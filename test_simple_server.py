#!/usr/bin/env python3
"""
Enhanced MCP Server for Article Quadrant Analysis with Chinese Support + OCR

Combines the working Chinese text matrix version with OCR capabilities.
"""

import asyncio
import logging
import sys
import os
import base64
import io
from typing import Dict, Any, Optional
from pydantic import BaseModel, Field
from pathlib import Path

try:
    from fastmcp import FastMCP, Context
except ImportError:
    print("FastMCP not found. Install with: pip install fastmcp")
    sys.exit(1)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize MCP server
mcp = FastMCP("Article Quadrant Analyzer (Enhanced + OCR)")

# OCR Support Functions
class MistralOCRProcessor:
    """Mistral Document AI API for OCR processing"""

    def __init__(self):
        self.api_key = os.getenv("MISTRAL_API_KEY")
        self.api_url = "https://api.mistral.ai/v1/ocr"

    async def extract_text_from_image(self, image_source: str) -> Dict[str, Any]:
        """
        Extract text from image using Mistral Document AI API

        Args:
            image_source: File path or base64 image data

        Returns:
            Dictionary with extracted text and metadata
        """
        if not self.api_key:
            return {
                "success": False,
                "error": "MISTRAL_API_KEY not configured",
                "text": "",
                "engine": "mistral_unavailable"
            }

        try:
            # Prepare image data
            if os.path.exists(image_source):
                # Read from file
                with open(image_source, 'rb') as f:
                    image_data = f.read()
                # Convert to base64
                base64_image = base64.b64encode(image_data).decode('utf-8')
            elif image_source.startswith('data:image'):
                # Already base64 encoded
                base64_image = image_source.split(',')[1] if ',' in image_source else image_source
            else:
                # Assume base64
                base64_image = image_source

            # Prepare API request
            import httpx

            payload = {
                "model": "mistral-ocr-latest",
                "image": f"data:image/png;base64,{base64_image}",
                "language": "auto"  # Auto-detect language
            }

            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            async with httpx.AsyncClient() as client:
                response = await client.post(self.api_url, json=payload, headers=headers)
                response.raise_for_status()
                result = response.json()

            # Extract text from response
            extracted_text = result.get("text", "")

            return {
                "success": True,
                "text": extracted_text,
                "engine": "mistral_ocr",
                "language": self._detect_language(extracted_text),
                "confidence": result.get("confidence", 0.0)
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"OCR processing failed: {str(e)}",
                "text": "",
                "engine": "mistral_error"
            }

    def _detect_language(self, text: str) -> str:
        """Simple language detection"""
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_chars = len([c for c in text if 'a' <= c.lower() <= 'z'])

        if chinese_chars > english_chars:
            return "ä¸­æ–‡"
        elif english_chars > 0:
            return "è‹±æ–‡"
        else:
            return "å…¶ä»–"

# Global OCR processor
ocr_processor = MistralOCRProcessor()

# Input models for validation
class TextInput(BaseModel):
    content: str = Field(..., description="Text content to analyze", min_length=1, max_length=50000)

class ImageInput(BaseModel):
    image_path: str = Field(..., description="Path to image file or base64 data")
    content_type: str = Field(default="image/png", description="Image content type")

@mcp.tool
async def extract_article_content_simple(
    ctx: Context,
    content: str
) -> str:
    """
    å¢å¼ºå†…å®¹æå–ï¼Œæ”¯æŒä¸­æ–‡å’Œå¤šç§æ ¼å¼

    Args:
        content: è¦å¤„ç†çš„åŸå§‹å†…å®¹
        ctx: MCPä¸Šä¸‹æ–‡

    Returns:
        å¤„ç†åçš„å†…å®¹å’ŒåŸºç¡€åˆ†æ
    """
    await ctx.info("æ­£åœ¨å¤„ç†æ–‡ç« å†…å®¹...")

    # å¢å¼ºå†…å®¹é¢„å¤„ç†
    import re

    # å¤„ç†å„ç§å†…å®¹æ ¼å¼
    if not content or not content.strip():
        await ctx.warning("æä¾›äº†ç©ºå†…å®¹")
        return "âŒ é”™è¯¯ï¼šæ²¡æœ‰æä¾›è¦åˆ†æçš„å†…å®¹ã€‚è¯·æä¾›æ–‡æœ¬ã€URLæˆ–æ–‡ä»¶å†…å®¹ã€‚"

    # æ¸…ç†å†…å®¹
    content = content.strip()

    # ç§»é™¤HTML/XMLæ ‡ç­¾ï¼ˆç½‘é¡µå†…å®¹ä¸­å¸¸è§ï¼‰
    content = re.sub(r'<[^>]+>', ' ', content)

    # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
    content = re.sub(r'\s+', ' ', content)

    # æå–å…³é”®æŒ‡æ ‡
    words = content.split()
    sentences = [s.strip() for s in content.split('.') if s.strip()]
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

    # å†…å®¹åˆ†æ
    def analyze_content_quality(text):
        """åˆ†æå†…å®¹è´¨é‡å’Œç‰¹å¾"""

        # è¯­è¨€æ£€æµ‹ï¼ˆç®€å•å¯å‘å¼ï¼‰
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))

        if chinese_chars > english_words:
            language = "ä¸­æ–‡"
        elif english_words > 0:
            language = "è‹±æ–‡"
        else:
            language = "æ··åˆ/å…¶ä»–"

        # å†…å®¹ç±»å‹æ£€æµ‹
        url_pattern = r'https?://[^\s]+'
        has_urls = bool(re.search(url_pattern, text))

        # é˜…è¯»å¤æ‚åº¦ï¼ˆç®€å•ä¼°è®¡ï¼‰
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0

        return {
            "language": language,
            "has_urls": has_urls,
            "avg_sentence_length": avg_sentence_length,
            "complexity": "é«˜" if avg_sentence_length > 20 else "ä¸­" if avg_sentence_length > 10 else "ä½"
        }

    content_analysis = analyze_content_quality(content)

    # ç”Ÿæˆç»¼åˆç»“æœ
    result = f"""ğŸ“„ **å†…å®¹æå–æˆåŠŸï¼**

**ğŸ“Š å†…å®¹æŒ‡æ ‡ï¼š**
- ğŸ“ å­—æ•°ç»Ÿè®¡: {len(words)} è¯
- ğŸ“„ å¥å­æ•°é‡: {len(sentences)} å¥
- ğŸ“‘ æ®µè½æ•°é‡: {len(paragraphs)} æ®µ
- ğŸŒ è¯­è¨€ç±»å‹: {content_analysis['language']}
- ğŸ¯ å†…å®¹å¤æ‚åº¦: {content_analysis['complexity']}

**ğŸ” å†…å®¹é¢„è§ˆï¼š**
{content[:200]}{'...' if len(content) > 200 else ''}

æ­¤å†…å®¹ç°å·²å‡†å¤‡å¥½è¿›è¡Œå››è±¡é™åˆ†æã€‚ä½¿ç”¨ `analyze_article_insights_simple` å·¥å…·æå–å…³é”®æ´å¯Ÿï¼Œæˆ–ç›´æ¥ä½¿ç”¨ `generate_quadrant_analysis_simple` è¿›è¡Œç»¼åˆå››è±¡é™æ˜ å°„ã€‚

**ğŸ¯ åˆ†æå»ºè®®ï¼š**
{'æ£€æµ‹åˆ°ä¸°å¯Œå†…å®¹ - é€‚åˆè¯¦ç»†å››è±¡é™åˆ†æ' if len(words) > 100 else 'æ£€æµ‹åˆ°ç®€æ´å†…å®¹ - é€‚åˆå¿«é€Ÿå››è±¡é™æ´å¯Ÿ'}
âœ… **å†…å®¹æå–æˆåŠŸå®Œæˆï¼**
"""
    await ctx.info(f"æˆåŠŸæå– {len(words)} è¯çš„ {content_analysis['language']} å†…å®¹")
    return result

@mcp.tool
async def extract_text_from_image(
    ctx: Context,
    image_path: str
) -> str:
    """
    ä½¿ç”¨Mistral Document AIä»å›¾ç‰‡ä¸­æå–æ–‡å­—ï¼ˆOCRï¼‰

    Args:
        image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„æˆ–base64ç¼–ç æ•°æ®
        ctx: MCPä¸Šä¸‹æ–‡

    Returns:
        æå–çš„æ–‡å­—å†…å®¹å’ŒOCRç»“æœ
    """
    await ctx.info("æ­£åœ¨è¿›è¡ŒOCRæ–‡å­—è¯†åˆ«...")

    try:
        # éªŒè¯å›¾ç‰‡æ–‡ä»¶
        if not image_path.startswith('data:image') and not os.path.exists(image_path):
            await ctx.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return f"âŒ é”™è¯¯ï¼šå›¾ç‰‡æ–‡ä»¶ '{image_path}' ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ã€‚"

        await ctx.info(f"æ­£åœ¨å¤„ç†å›¾ç‰‡: {image_path[:50]}{'...' if len(image_path) > 50 else ''}")

        # æ‰§è¡ŒOCRå¤„ç†
        ocr_result = await ocr_processor.extract_text_from_image(image_path)

        if ocr_result["success"]:
            extracted_text = ocr_result["text"]
            language = ocr_result["language"]
            confidence = ocr_result.get("confidence", 0.0)

            # åˆ†ææå–çš„æ–‡å­—
            word_count = len(extracted_text.split())

            result = f"""ğŸ” **OCRæ–‡å­—è¯†åˆ«æˆåŠŸï¼**

**ğŸ“Š è¯†åˆ«ç»“æœï¼š**
- ğŸŒ æ£€æµ‹è¯­è¨€: {language}
- ğŸ“ è¯†åˆ«å­—æ•°: {word_count} è¯
- ğŸ¯ è¯†åˆ«ç½®ä¿¡åº¦: {confidence:.2f}
- ğŸ”§ OCRå¼•æ“: {ocr_result['engine']}

**ğŸ“„ æå–çš„æ–‡å­—å†…å®¹ï¼š**
```
{extracted_text[:500]}{'...' if len(extracted_text) > 500 else ''}
```

âœ… **OCRå¤„ç†å®Œæˆï¼** ç°åœ¨å¯ä»¥ä½¿ç”¨ `generate_quadrant_analysis_simple` å¯¹æå–çš„å†…å®¹è¿›è¡Œå››è±¡é™åˆ†æã€‚

**ğŸ’¡ å»ºè®®ï¼š**
- å¦‚æœè¯†åˆ«ç»“æœä¸ç†æƒ³ï¼Œè¯·ç¡®ä¿å›¾ç‰‡æ¸…æ™°ã€æ–‡å­—ç«¯æ­£
- æ”¯æŒä¸­æ–‡ã€è‹±æ–‡åŠæ··åˆè¯­è¨€è¯†åˆ«
- æ¨èä½¿ç”¨é«˜åˆ†è¾¨ç‡å›¾ç‰‡ä»¥è·å¾—æœ€ä½³è¯†åˆ«æ•ˆæœ
"""
            await ctx.info(f"æˆåŠŸè¯†åˆ« {word_count} è¯çš„ {language} å†…å®¹")
            return result

        else:
            error_msg = ocr_result.get("error", "OCRå¤„ç†å¤±è´¥")
            await ctx.warning(f"OCRå¤„ç†å¤±è´¥: {error_msg}")

            if "MISTRAL_API_KEY" in error_msg:
                return f"""âŒ **OCRæœåŠ¡æœªé…ç½®**

**é”™è¯¯ä¿¡æ¯ï¼š** {error_msg}

**è§£å†³æ–¹æ¡ˆï¼š**
1. è·å– Mistral API Key: https://console.mistral.ai/
2. è®¾ç½®ç¯å¢ƒå˜é‡: `export MISTRAL_API_KEY=your_api_key_here`
3. é‡å¯MCPæœåŠ¡å™¨

**ä¸´æ—¶æ–¹æ¡ˆï¼š**
- å¯ä»¥ç›´æ¥ä½¿ç”¨ `extract_article_content_simple` å¤„ç†æ–‡æœ¬å†…å®¹
- æˆ–æ‰‹åŠ¨è¾“å…¥æ–‡å­—å†…å®¹è¿›è¡Œåˆ†æ
"""
            else:
                return f"""âŒ **OCRå¤„ç†å¤±è´¥**

**é”™è¯¯ä¿¡æ¯ï¼š** {error_msg}

**å¯èƒ½çš„åŸå› ï¼š**
- å›¾ç‰‡æ ¼å¼ä¸æ”¯æŒï¼ˆæ¨èPNG, JPGï¼‰
- å›¾ç‰‡è´¨é‡è¿‡ä½æˆ–æ¨¡ç³Š
- ç½‘ç»œè¿æ¥é—®é¢˜
- APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨

**å»ºè®®ï¼š**
- å°è¯•ä½¿ç”¨æ›´æ¸…æ™°çš„å›¾ç‰‡
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ç¨åé‡è¯•æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬å†…å®¹
"""

    except Exception as e:
        await ctx.error(f"OCRå¤„ç†å¼‚å¸¸: {str(e)}")
        return f"âŒ OCRå¤„ç†å‘ç”Ÿå¼‚å¸¸: {str(e)}\n\nè¯·æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æˆ–ç¨åé‡è¯•ã€‚"

@mcp.tool
async def analyze_article_insights_simple(
    ctx: Context,
    content: str
) -> str:
    """
    å¢å¼ºå†…å®¹æ´å¯Ÿåˆ†æï¼Œæ”¯æŒä¸­æ–‡å…³é”®è¯æå–

    Args:
        content: è¦åˆ†æçš„æ–‡æœ¬å†…å®¹
        ctx: MCPä¸Šä¸‹æ–‡

    Returns:
        ä»å†…å®¹ä¸­æå–çš„åŸºç¡€æ´å¯Ÿ
    """
    await ctx.info("æ­£åœ¨åˆ†ææ–‡ç« æ´å¯Ÿ...")

    # ç®€å•NLPæ¨¡æ‹Ÿ
    words = content.lower().split()
    word_freq = {}
    for word in words:
        if len(word) > 3:  # è·³è¿‡çŸ­è¯
            word_freq[word] = word_freq.get(word, 0) + 1

    # è·å–é«˜é¢‘è¯
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]

    result = f"""ğŸ§  **æ–‡ç« æ´å¯Ÿåˆ†æï¼**

**ğŸ” å…³é”®ä¸»é¢˜ï¼š**
{chr(10).join([f"- {word}: {count} æ¬¡æåŠ" for word, count in top_words])}

**ğŸ“ˆ å†…å®¹ç‰¹å¾ï¼š**
- å†…å®¹é•¿åº¦: {'é•¿' if len(content) > 500 else 'ä¸­' if len(content) > 200 else 'çŸ­'}
- è¯æ•°ç»Ÿè®¡: {len(words)} è¯
- å¥å­åˆ†æ: {len(content.split('.'))} å¥

**ğŸ’¡ åŸºç¡€æ´å¯Ÿï¼š**
å†…å®¹åŒ…å« {len(top_words)} ä¸ªä¸»è¦ä¸»é¢˜ï¼Œé€‚åˆè¿›è¡Œå››è±¡é™æˆ˜ç•¥åˆ†æã€‚

**ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
ä½¿ç”¨ `generate_quadrant_analysis_simple` è¿›è¡Œæœ€ç»ˆçš„å››è±¡é™å¯è§†åŒ–åˆ†æã€‚
"""
    await ctx.info("æ´å¯Ÿåˆ†æå®Œæˆ")
    return result

@mcp.tool
async def generate_quadrant_analysis_simple(
    ctx: Context,
    content: str,
    x_axis_label: str = "å½±å“åŠ›",
    y_axis_label: str = "éš¾åº¦"
) -> str:
    """
    ç”Ÿæˆæ™ºèƒ½ä¸­æ–‡å››è±¡é™åˆ†æï¼Œç›´æ¥è¾“å‡ºå¯è§†åŒ–çŸ©é˜µ

    Args:
        content: è¦åˆ†æçš„å†…å®¹
        x_axis_label: Xè½´æ ‡ç­¾
        y_axis_label: Yè½´æ ‡ç­¾
        ctx: MCPä¸Šä¸‹æ–‡

    Returns:
        åŒ…å«ç›´æ¥æ˜¾ç¤ºçŸ©é˜µå›¾çš„ç»¼åˆåˆ†æ
    """
    await ctx.info("å¼€å§‹ç”Ÿæˆå››è±¡é™åˆ†æ...")

    # å¢å¼ºå†…å®¹é¢„å¤„ç†
    import re

    # æ¸…ç†å’Œæ ‡å‡†åŒ–å†…å®¹
    content = re.sub(r'<[^>]+>', '', content)  # ç§»é™¤HTML/XMLæ ‡ç­¾
    content = content.strip()

    if not content:
        await ctx.warning("æä¾›äº†ç©ºå†…å®¹")
        return "âŒ é”™è¯¯ï¼šæ²¡æœ‰æä¾›è¦åˆ†æçš„å†…å®¹ã€‚è¯·æä¾›æ–‡æœ¬æ¥åˆ†æã€‚"

    # æå–å…³é”®ä¸»é¢˜å’Œæ¦‚å¿µ
    words = content.lower().split()
    sentences = [s.strip() for s in content.split('.') if s.strip()]

    # æ™ºèƒ½å››è±¡é™åˆ†ç±»
    def analyze_content_for_quadrants(text):
        """åŸºäºå†…å®¹åˆ†æç”Ÿæˆæœ‰æ„ä¹‰çš„å››è±¡é™åˆ†ç±»ï¼ˆä¸­æ–‡ç‰ˆï¼‰"""
        # ä¸­æ–‡æˆ˜ç•¥æ¨¡å¼ï¼ˆåŸºäºæ‚¨çš„ç¤ºä¾‹ï¼‰
        # å¯¹äº "åä½œç¨‹åº¦" vs "æ–‡æœ¬åŒ–ç¨‹åº¦" çš„åˆ†æ
        quadrant_items = {
            "Q1": [],  # å³ä¸Š: é«˜åä½œç¨‹åº¦, é«˜æ–‡æœ¬åŒ–ç¨‹åº¦
            "Q2": [],  # å·¦ä¸Š: ä½åä½œç¨‹åº¦, é«˜æ–‡æœ¬åŒ–ç¨‹åº¦
            "Q3": [],  # å·¦ä¸‹: ä½åä½œç¨‹åº¦, ä½æ–‡æœ¬åŒ–ç¨‹åº¦
            "Q4": []   # å³ä¸‹: é«˜åä½œç¨‹åº¦, ä½æ–‡æœ¬åŒ–ç¨‹åº¦
        }

        # æå–æœ‰æ„ä¹‰çš„çŸ­è¯­å¹¶è¿›è¡Œåˆ†ç±»
        for sentence in sentences:
            sentence_lower = sentence.lower()

            # ä¸­æ–‡å…³é”®è¯åˆ†æ
            collaboration_keywords = ['å›¢é˜Ÿ', 'åä½œ', 'è®¨è®º', 'ä¼šè®®', 'æ²Ÿé€š', 'åˆ†äº«', 'å…±åŒ', 'ä¸€èµ·', 'åˆä½œ', 'é›†ä½“']
            text_keywords = ['æ–‡æ¡£', 'æŠ¥å‘Š', 'è®°å½•', 'æ’°å†™', 'ç¼–å†™', 'åˆ†æ', 'ç ”ç©¶', 'é˜…è¯»', 'æ€»ç»“', 'è§„åˆ’']

            # è®¡ç®—åä½œç¨‹åº¦å’Œæ–‡æœ¬åŒ–ç¨‹åº¦
            collaboration_score = sum(1 for keyword in collaboration_keywords if keyword in sentence_lower)
            text_score = sum(1 for keyword in text_keywords if keyword in sentence_lower)

            # æ ¹æ®åˆ†æ•°åˆ†ç±»åˆ°è±¡é™
            if collaboration_score >= 2 and text_score >= 2:
                quadrant_items["Q1"].append(sentence)
            elif collaboration_score < 2 and text_score >= 2:
                quadrant_items["Q2"].append(sentence)
            elif collaboration_score < 2 and text_score < 2:
                quadrant_items["Q3"].append(sentence)
            else:  # collaboration_score >= 2 and text_score < 2
                quadrant_items["Q4"].append(sentence)

        # å¦‚æœæŸä¸ªè±¡é™ä¸ºç©ºï¼Œæ·»åŠ é»˜è®¤å†…å®¹
        if not quadrant_items["Q1"]:
            quadrant_items["Q1"] = ["å›¢é˜Ÿåä½œæ–‡æ¡£", "é›†ä½“è®¨è®ºè®°å½•", "å…±äº«æˆæœå±•ç¤º"]
        if not quadrant_items["Q2"]:
            quadrant_items["Q2"] = ["ç‹¬ç«‹æ·±åº¦æ€è€ƒ", "ä¸ªäººä¸“ä¸šåˆ†æ", "æ ¸å¿ƒæŠ€æœ¯å®ç°"]
        if not quadrant_items["Q3"]:
            quadrant_items["Q3"] = ["åŸºç¡€ç»´æŠ¤å·¥ä½œ", "å¸¸è§„æ“ä½œæµç¨‹", "æ ‡å‡†è§„èŒƒæ‰§è¡Œ"]
        if not quadrant_items["Q4"]:
            quadrant_items["Q4"] = ["åˆ›æ„å¤´è„‘é£æš´", "è§†è§‰åŒ–è¡¨è¾¾", "äº’åŠ¨åä½œå±•ç¤º"]

        return quadrant_items

    # åˆ†æå†…å®¹
    quadrants = analyze_content_for_quadrants(content)

    # ç”Ÿæˆç›´æ¥æ˜¾ç¤ºçš„æ–‡æœ¬å››è±¡é™å›¾
    def generate_text_quadrant(quadrants, x_axis_label, y_axis_label):
        """ç”Ÿæˆç›´æ¥æ˜¾ç¤ºçš„æ–‡æœ¬å››è±¡é™å›¾"""
        # å››è±¡é™çš„ä¸­æ–‡åç§°
        quadrant_names = {
            "Q1": "é‡ç‚¹æŠ•å…¥åŒº",
            "Q2": "ä¸“ä¸šåˆ†æåŒº",
            "Q3": "åŸºç¡€ç»´æŠ¤åŒº",
            "Q4": "åˆ›æ„åä½œåŒº"
        }

        # æ ¼å¼åŒ–è±¡é™å†…å®¹
        def format_quadrant_content(items):
            if not items:
                return "â€¢ å¾…åˆ†æå†…å®¹"
            formatted_items = []
            for item in items[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                if len(item) > 12:
                    formatted_items.append(f"â€¢ {item[:12]}...")
                else:
                    formatted_items.append(f"â€¢ {item}")
            return '\nâ”‚  â”‚ '.join(formatted_items) if formatted_items else "â€¢ å¾…åˆ†æå†…å®¹"

        diagram = f"""
**ğŸ¯ å››è±¡é™çŸ©é˜µå›¾**
```
                    â†‘ {y_axis_label} â†‘
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Q1: {quadrant_names['Q1']}     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ {format_quadrant_content(quadrants['Q1'])}  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Q2: {quadrant_names['Q2']}     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ {format_quadrant_content(quadrants['Q2'])}  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â† {x_axis_label} â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ {x_axis_label} â†’
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Q3: {quadrant_names['Q3']}     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ {format_quadrant_content(quadrants['Q3'])}  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Q4: {quadrant_names['Q4']}     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ {format_quadrant_content(quadrants['Q4'])}  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
"""
        return diagram

    quadrant_diagram = generate_text_quadrant(quadrants, x_axis_label, y_axis_label)

    # ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
    result = f"""ğŸ¯ **å››è±¡é™åˆ†æå®Œæˆï¼**

{quadrant_diagram}

**ğŸ“Š è±¡é™è¯´æ˜ï¼š**
- **Q1: é‡ç‚¹æŠ•å…¥åŒº** - é«˜åä½œç¨‹åº¦ + é«˜æ–‡æœ¬åŒ–ç¨‹åº¦
- **Q2: ä¸“ä¸šåˆ†æåŒº** - ä½åä½œç¨‹åº¦ + é«˜æ–‡æœ¬åŒ–ç¨‹åº¦
- **Q3: åŸºç¡€ç»´æŠ¤åŒº** - ä½åä½œç¨‹åº¦ + ä½æ–‡æœ¬åŒ–ç¨‹åº¦
- **Q4: åˆ›æ„åä½œåŒº** - é«˜åä½œç¨‹åº¦ + ä½æ–‡æœ¬åŒ–ç¨‹åº¦

**ğŸ” å†…å®¹åˆ†ææ‘˜è¦ï¼š**
- å¤„ç†å†…å®¹: {len(words)} è¯
- è¯†åˆ«å¥å­: {len(sentences)} å¥
- åˆ†æç»´åº¦: åä½œç¨‹åº¦ vs æ–‡æœ¬åŒ–ç¨‹åº¦

**ğŸ’¡ æˆ˜ç•¥å»ºè®®ï¼š**
- é‡ç‚¹å…³æ³¨Q1åŒºåŸŸçš„é¡¹ç›®ï¼Œéœ€è¦å›¢é˜Ÿåä½œå’Œæ–‡æ¡£è®°å½•
- åˆ©ç”¨Q2åŒºåŸŸçš„ä¸“ä¸šèƒ½åŠ›ï¼Œè¿›è¡Œæ·±åº¦åˆ†æå·¥ä½œ
- ä¼˜åŒ–Q3åŒºåŸŸçš„åŸºç¡€æµç¨‹ï¼Œæé«˜æ•ˆç‡
- å‘æŒ¥Q4åŒºåŸŸçš„åˆ›æ„ä¼˜åŠ¿ï¼Œä¿ƒè¿›å›¢é˜Ÿåä½œ

**ğŸš€ å®ŒæˆçŠ¶æ€ï¼š** å››è±¡é™åˆ†æå·²ç”Ÿæˆï¼Œå¯ç”¨äºæˆ˜ç•¥å†³ç­–å’Œé¡¹ç›®è§„åˆ’
"""

    await ctx.info("âœ… å››è±¡é™åˆ†æç”Ÿæˆå®Œæˆï¼")
    return result

if __name__ == "__main__":
    print("ğŸš€ Starting Enhanced Article Quadrant Analyzer MCP Server")
    print("âœ¨ Features: Chinese Support + OCR + Direct Text Matrix Output")
    print("ğŸ“Š Tools: Content Extraction, OCR, Insights Analysis, Quadrant Generation")
    print("ğŸ‡¨ğŸ‡³ Enhanced with Chinese Language Support and Mistral OCR Integration")

    # Run the server
    mcp.run()