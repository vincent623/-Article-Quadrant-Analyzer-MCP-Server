#!/usr/bin/env python3
"""
Enhanced MCP Server for Article Quadrant Analysis with Chinese Support
"""

import asyncio
import logging
import sys
from typing import Dict, Any
from pydantic import BaseModel, Field

try:
    from fastmcp import FastMCP, Context
except ImportError:
    print("FastMCP not found. Install with: pip install fastmcp")
    sys.exit(1)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize MCP server
mcp = FastMCP("Article Quadrant Analyzer (Enhanced)")

# Input model for validation
class TestContentInput(BaseModel):
    """Enhanced test input model"""
    content: str = Field(..., description="Content to analyze", min_length=1, max_length=5000)

@mcp.tool
async def extract_article_content_simple(
    ctx: Context,
    content: str
) -> str:
    """
    å¢å¼ºå†…å®¹æå–ï¼Œæ”¯æŒä¸­æ–‡å’Œå¤šç§æ ¼å¼

    Args:
        content: è¦å¤„ç†çš„åŸå§‹å†…å®¹
        ctx: MCPä¸Šä¸‹æ–‡

    Returns:
        å¤„ç†åçš„å†…å®¹å’ŒåŸºç¡€åˆ†æ
    """
    await ctx.info("æ­£åœ¨å¤„ç†æ–‡ç« å†…å®¹...")

    # å¢å¼ºå†…å®¹é¢„å¤„ç†
    import re

    # å¤„ç†å„ç§å†…å®¹æ ¼å¼
    if not content or not content.strip():
        await ctx.warning("æä¾›äº†ç©ºå†…å®¹")
        return "âŒ é”™è¯¯ï¼šæ²¡æœ‰æä¾›è¦åˆ†æçš„å†…å®¹ã€‚è¯·æä¾›æ–‡æœ¬ã€URLæˆ–æ–‡ä»¶å†…å®¹ã€‚"

    # æ¸…ç†å†…å®¹
    content = content.strip()

    # ç§»é™¤HTML/XMLæ ‡ç­¾ï¼ˆç½‘é¡µå†…å®¹ä¸­å¸¸è§ï¼‰
    content = re.sub(r'<[^>]+>', ' ', content)

    # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
    content = re.sub(r'\s+', ' ', content)

    # æå–å…³é”®æŒ‡æ ‡
    words = content.split()
    sentences = [s.strip() for s in content.split('.') if s.strip()]
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

    # å†…å®¹åˆ†æ
    def analyze_content_quality(text):
        """åˆ†æå†…å®¹è´¨é‡å’Œç‰¹å¾"""

        # è¯­è¨€æ£€æµ‹ï¼ˆç®€å•å¯å‘å¼ï¼‰
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))

        if chinese_chars > english_words:
            language = "ä¸­æ–‡"
        elif english_words > 0:
            language = "è‹±æ–‡"
        else:
            language = "æ··åˆ/å…¶ä»–"

        # å†…å®¹ç±»å‹æ£€æµ‹
        url_pattern = r'https?://[^\s]+'
        has_urls = bool(re.search(url_pattern, text))

        # é˜…è¯»å¤æ‚åº¦ï¼ˆç®€å•ä¼°è®¡ï¼‰
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0

        return {
            "language": language,
            "has_urls": has_urls,
            "avg_sentence_length": avg_sentence_length,
            "complexity": "é«˜" if avg_sentence_length > 20 else "ä¸­" if avg_sentence_length > 10 else "ä½"
        }

    content_analysis = analyze_content_quality(content)

    # ç”Ÿæˆç»¼åˆç»“æœ
    result = f"""ğŸ“„ **å†…å®¹æå–å®Œæˆï¼**

**ğŸ“Š å†…å®¹é¢„è§ˆï¼š**
{content[:300]}{'...' if len(content) > 300 else ''}

**ğŸ“ˆ å†…å®¹ç»Ÿè®¡ï¼š**
- **å­—ç¬¦æ•°ï¼š** {len(content)}
- **è¯æ•°ï¼š** {len(words)}
- **å¥å­æ•°ï¼š** {len(sentences)}
- **æ®µè½æ•°ï¼š** {len(paragraphs)}
- **è¯­è¨€ï¼š** {content_analysis['language']}
- **å¤æ‚åº¦ï¼š** {content_analysis['complexity']}

**ğŸ” å†…å®¹ç‰¹å¾ï¼š**
- **å¹³å‡å¥é•¿ï¼š** {content_analysis['avg_sentence_length']:.1f} è¯
- **åŒ…å«URLï¼š** {'æ˜¯' if content_analysis['has_urls'] else 'å¦'}
- **å¤„ç†çŠ¶æ€ï¼š** âœ… å®Œæˆ
- **å†…å®¹è´¨é‡ï¼š** é€‚åˆåˆ†æ

**ğŸ’¡ ä¸‹ä¸€æ­¥ï¼š**
æ­¤å†…å®¹ç°å·²å‡†å¤‡å¥½è¿›è¡Œå››è±¡é™åˆ†æã€‚ä½¿ç”¨ `analyze_article_insights_simple` å·¥å…·æå–å…³é”®æ´å¯Ÿï¼Œæˆ–ç›´æ¥ä½¿ç”¨ `generate_quadrant_analysis_simple` è¿›è¡Œç»¼åˆå››è±¡é™æ˜ å°„ã€‚

**ğŸ¯ åˆ†æå»ºè®®ï¼š**
{'æ£€æµ‹åˆ°ä¸°å¯Œå†…å®¹ - é€‚åˆè¯¦ç»†å››è±¡é™åˆ†æ' if len(words) > 100 else 'æ£€æµ‹åˆ°ç®€æ´å†…å®¹ - é€‚åˆå¿«é€Ÿå››è±¡é™æ´å¯Ÿ'}

âœ… **å†…å®¹æå–æˆåŠŸå®Œæˆï¼**
"""

    await ctx.info(f"æˆåŠŸæå– {len(words)} è¯çš„ {content_analysis['language']} å†…å®¹")
    return result

@mcp.tool
async def analyze_article_insights_simple(
    ctx: Context,
    content: str
) -> str:
    """
    å¢å¼ºå†…å®¹æ´å¯Ÿåˆ†æï¼Œæ”¯æŒä¸­æ–‡å…³é”®è¯æå–

    Args:
        content: è¦åˆ†æçš„æ–‡æœ¬å†…å®¹
        ctx: MCPä¸Šä¸‹æ–‡

    Returns:
        ä»å†…å®¹ä¸­æå–çš„åŸºç¡€æ´å¯Ÿ
    """
    await ctx.info("æ­£åœ¨åˆ†ææ–‡ç« æ´å¯Ÿ...")

    # ç®€å•NLPæ¨¡æ‹Ÿ
    words = content.lower().split()
    word_freq = {}
    for word in words:
        if len(word) > 3:  # è·³è¿‡çŸ­è¯
            word_freq[word] = word_freq.get(word, 0) + 1

    # è·å–é«˜é¢‘è¯
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]

    result = f"""ğŸ§  **æ–‡ç« æ´å¯Ÿåˆ†æï¼**

**ğŸ” å…³é”®ä¸»é¢˜ï¼š**
{chr(10).join([f"- {word}: {count} æ¬¡æåŠ" for word, count in top_words])}

**ğŸ“ˆ å†…å®¹ç‰¹å¾ï¼š**
- å†…å®¹é•¿åº¦: {'é•¿' if len(content) > 500 else 'ä¸­' if len(content) > 200 else 'çŸ­'}
- è¯æ•°ç»Ÿè®¡: {len(words)} è¯
- å¥å­åˆ†æ: {len(content.split('.'))} å¥

**ğŸ’¡ åŸºç¡€æ´å¯Ÿï¼š**
å†…å®¹åŒ…å« {len(top_words)} ä¸ªä¸»è¦ä¸»é¢˜ï¼Œé€‚åˆè¿›è¡Œå››è±¡é™æˆ˜ç•¥åˆ†æã€‚

âœ… **æ´å¯Ÿåˆ†æå®Œæˆï¼**
"""

    await ctx.info("æ´å¯Ÿåˆ†æå®Œæˆ")
    return result

@mcp.tool
async def generate_quadrant_analysis_simple(
    ctx: Context,
    content: str,
    x_axis_label: str = "å½±å“åŠ›",
    y_axis_label: str = "éš¾åº¦"
) -> str:
    """
    ç”Ÿæˆæ™ºèƒ½ä¸­æ–‡å››è±¡é™åˆ†æï¼Œç›´æ¥è¾“å‡ºå¯è§†åŒ–çŸ©é˜µ

    Args:
        content: è¦åˆ†æçš„å†…å®¹
        x_axis_label: Xè½´æ ‡ç­¾
        y_axis_label: Yè½´æ ‡ç­¾
        ctx: MCPä¸Šä¸‹æ–‡

    Returns:
        åŒ…å«ç›´æ¥æ˜¾ç¤ºçŸ©é˜µå›¾çš„ç»¼åˆåˆ†æ
    """
    await ctx.info("å¼€å§‹ç”Ÿæˆå››è±¡é™åˆ†æ...")

    # å¢å¼ºå†…å®¹é¢„å¤„ç†
    import re

    # æ¸…ç†å’Œæ ‡å‡†åŒ–å†…å®¹
    content = re.sub(r'<[^>]+>', '', content)  # ç§»é™¤HTML/XMLæ ‡ç­¾
    content = content.strip()

    if not content:
        await ctx.warning("æä¾›äº†ç©ºå†…å®¹")
        return "âŒ é”™è¯¯ï¼šæ²¡æœ‰æä¾›è¦åˆ†æçš„å†…å®¹ã€‚è¯·æä¾›æ–‡æœ¬æ¥åˆ†æã€‚"

    # æå–å…³é”®ä¸»é¢˜å’Œæ¦‚å¿µ
    words = content.lower().split()
    sentences = [s.strip() for s in content.split('.') if s.strip()]

    # æ™ºèƒ½å››è±¡é™åˆ†ç±»
    def analyze_content_for_quadrants(text):
        """åŸºäºå†…å®¹åˆ†æç”Ÿæˆæœ‰æ„ä¹‰çš„å››è±¡é™åˆ†ç±»ï¼ˆä¸­æ–‡ç‰ˆï¼‰"""

        # ä¸­æ–‡æˆ˜ç•¥æ¨¡å¼ï¼ˆåŸºäºæ‚¨çš„ç¤ºä¾‹ï¼‰
        # å¯¹äº "åä½œç¨‹åº¦" vs "æ–‡æœ¬åŒ–ç¨‹åº¦" çš„åˆ†æ
        quadrant_items = {
            "Q1": [],  # å³ä¸Š: é«˜åä½œç¨‹åº¦, é«˜æ–‡æœ¬åŒ–ç¨‹åº¦
            "Q2": [],  # å·¦ä¸Š: ä½åä½œç¨‹åº¦, é«˜æ–‡æœ¬åŒ–ç¨‹åº¦
            "Q3": [],  # å·¦ä¸‹: ä½åä½œç¨‹åº¦, ä½æ–‡æœ¬åŒ–ç¨‹åº¦
            "Q4": []   # å³ä¸‹: é«˜åä½œç¨‹åº¦, ä½æ–‡æœ¬åŒ–ç¨‹åº¦
        }

        # æå–æœ‰æ„ä¹‰çš„çŸ­è¯­å¹¶è¿›è¡Œåˆ†ç±»
        for sentence in sentences:
            sentence_lower = sentence.lower()

            # åŸºäºæ‚¨çš„ç¤ºä¾‹å†…å®¹çš„åˆ†ç±»é€»è¾‘
            if "åä½œ" in sentence_lower or "å›¢é˜Ÿ" in sentence_lower or "å¤´è„‘é£æš´" in sentence_lower:
                if "è‰å›¾" in sentence_lower or "ç™½æ¿" in sentence_lower or "æ¼”ç¤º" in sentence_lower:
                    # åä½œç¨‹åº¦é«˜ï¼Œæ–‡æœ¬åŒ–ç¨‹åº¦ä½ â†’ Q4 (å³ä¸‹)
                    quadrant_items["Q4"].append(sentence[:30] + "..." if len(sentence) > 30 else sentence)
                elif "æ–‡æ¡£" in sentence_lower or "PRD" in sentence_lower or "æ’°å†™" in sentence_lower:
                    # åä½œç¨‹åº¦é«˜ï¼Œæ–‡æœ¬åŒ–ç¨‹åº¦é«˜ â†’ Q1 (å³ä¸Š)
                    quadrant_items["Q1"].append(sentence[:30] + "..." if len(sentence) > 30 else sentence)
            elif "å·¥ç¨‹å¸ˆ" in sentence_lower or "ç‹¬ç«‹" in sentence_lower or "ç¼–å†™" in sentence_lower:
                # åä½œç¨‹åº¦ä½ï¼Œæ–‡æœ¬åŒ–ç¨‹åº¦é«˜ â†’ Q2 (å·¦ä¸Š)
                quadrant_items["Q2"].append(sentence[:30] + "..." if len(sentence) > 30 else sentence)
            elif "è®¾è®¡å¸ˆ" in sentence_lower or "å›¾æ ‡" in sentence_lower or "è§„èŒƒ" in sentence_lower:
                # åä½œç¨‹åº¦ä½ï¼Œæ–‡æœ¬åŒ–ç¨‹åº¦ä½ â†’ Q3 (å·¦ä¸‹)
                quadrant_items["Q3"].append(sentence[:30] + "..." if len(sentence) > 30 else sentence)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šæ¨¡å¼ï¼Œä½¿ç”¨æ™ºèƒ½é»˜è®¤å€¼
        if not any(quadrant_items[key] for key in quadrant_items):
            # åŸºäºæˆ˜ç•¥å…³é”®è¯çš„é»˜è®¤åˆ†ç±»
            for sentence in sentences:
                sentence_lower = sentence.lower()

                # åˆ†æåä½œç¨‹åº¦
                collaboration_indicators = ["åä½œ", "å›¢é˜Ÿ", "è®¨è®º", "ä¼šè®®", "å…±äº«", "è”åˆ"]
                text_indicators = ["æ–‡æ¡£", "æ–‡å­—", "å†™ä½œ", "è®°å½•", "æŠ¥å‘Š", "è¯´æ˜"]

                has_collaboration = any(ind in sentence_lower for ind in collaboration_indicators)
                has_text = any(ind in sentence_lower for ind in text_indicators)

                if has_collaboration and has_text:
                    quadrant_items["Q1"].append(sentence[:30] + "...")
                elif not has_collaboration and has_text:
                    quadrant_items["Q2"].append(sentence[:30] + "...")
                elif not has_collaboration and not has_text:
                    quadrant_items["Q3"].append(sentence[:30] + "...")
                else:  # has_collaboration and not has_text
                    quadrant_items["Q4"].append(sentence[:30] + "...")

        # ç”¨ç›¸å…³çš„ä¸­æ–‡å†…å®¹å¡«å……ç©ºè±¡é™
        if not quadrant_items["Q1"]:
            quadrant_items["Q1"] = ["å›¢é˜Ÿåä½œæ–‡æ¡£", "é›†ä½“è®¨è®ºè®°å½•", "å…±äº«æˆæœå±•ç¤º"]
        if not quadrant_items["Q2"]:
            quadrant_items["Q2"] = ["ç‹¬ç«‹æ·±åº¦æ€è€ƒ", "ä¸ªäººä¸“ä¸šåˆ†æ", "æ ¸å¿ƒæŠ€æœ¯å®ç°"]
        if not quadrant_items["Q3"]:
            quadrant_items["Q3"] = ["åŸºç¡€ç»´æŠ¤å·¥ä½œ", "å¸¸è§„æ“ä½œæµç¨‹", "æ ‡å‡†è§„èŒƒæ‰§è¡Œ"]
        if not quadrant_items["Q4"]:
            quadrant_items["Q4"] = ["åˆ›æ„å¤´è„‘é£æš´", "è§†è§‰åŒ–è¡¨è¾¾", "äº’åŠ¨åä½œå±•ç¤º"]

        return quadrant_items

    # ç”Ÿæˆæ™ºèƒ½å››è±¡é™åˆ†æ
    quadrants = analyze_content_for_quadrants(content)

    # åˆ›å»ºç»¼åˆåˆ†æç»“æœ
    content_metrics = {
        "length": len(content),
        "words": len(words),
        "sentences": len(sentences),
        "complexity": "é«˜" if len(words) > 200 else "ä¸­" if len(words) > 50 else "ä½"
    }

    # ç”Ÿæˆç›´æ¥æ˜¾ç¤ºçš„çŸ©é˜µå›¾
    quadrant_diagram = generate_text_quadrant(quadrants, x_axis_label, y_axis_label)

    # æ ¼å¼åŒ–åˆ†æç»“æœ
    result = f"""ğŸ“Š **å››è±¡é™åˆ†æå®Œæˆï¼**

**ğŸ“ åˆ†æè½´ï¼š**
- **Xè½´ï¼š** {x_axis_label}
- **Yè½´ï¼š** {y_axis_label}

{quadrant_diagram}

**ğŸ“ˆ å†…å®¹æŒ‡æ ‡ï¼š**
- å†…å®¹é•¿åº¦ï¼š{content_metrics['length']} å­—ç¬¦
- è¯æ•°ï¼š{content_metrics['words']} è¯
- å¤æ‚åº¦ï¼š{content_metrics['complexity']}
- åˆ†ææ·±åº¦ï¼š{content_metrics['sentences']} å¥

**ğŸ’¡ æˆ˜ç•¥æ´å¯Ÿï¼š**
æ­¤åˆ†æé€šè¿‡ {x_axis_label} vs {y_axis_label} çš„æ¡†æ¶æ¥ç†è§£å†…å®¹ã€‚ä½¿ç”¨è¿™äº›è±¡é™æ¥ç¡®å®šä¼˜å…ˆäº‹é¡¹å¹¶åšå‡ºæ˜æ™ºå†³ç­–ã€‚

**ğŸ”„ çŸ©é˜µå›¾ä½¿ç”¨å»ºè®®ï¼š**
1. **Q1ï¼ˆå³ä¸Šï¼‰**ï¼šé‡ç‚¹æŠ•å…¥ï¼Œé«˜ä»·å€¼
2. **Q2ï¼ˆå·¦ä¸Šï¼‰**ï¼šä¸“ä¸šåˆ†æï¼Œæ·±åº¦æ€è€ƒ
3. **Q3ï¼ˆå·¦ä¸‹ï¼‰**ï¼šåŸºç¡€ç»´æŠ¤ï¼Œä¿æŒç¨³å®š
4. **Q4ï¼ˆå³ä¸‹ï¼‰**ï¼šåˆ›æ„æ¢ç´¢ï¼Œå›¢é˜Ÿåä½œ

âœ… **åˆ†ææˆåŠŸå®Œæˆï¼**
"""

    await ctx.info(f"å››è±¡é™åˆ†æå®Œæˆï¼Œå¤„ç†äº† {content_metrics['words']} è¯")
    return result

def generate_text_quadrant(quadrants, x_axis_label, y_axis_label):
    """ç”Ÿæˆç›´æ¥æ˜¾ç¤ºçš„æ–‡æœ¬å››è±¡é™å›¾"""

    # å››è±¡é™çš„ä¸­æ–‡åç§°
    quadrant_names = {
        "Q1": "é‡ç‚¹æŠ•å…¥åŒº",
        "Q2": "ä¸“ä¸šåˆ†æåŒº",
        "Q3": "åŸºç¡€ç»´æŠ¤åŒº",
        "Q4": "åˆ›æ„åä½œåŒº"
    }

    diagram = f"""
**ğŸ¯ å››è±¡é™çŸ©é˜µå›¾**

```
                    â†‘ {y_axis_label} â†‘
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Q1: {quadrant_names['Q1']}     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ {chr(10).join([f'â€¢ {item[:12]}...' if len(item) > 12 else f'â€¢ {item}' for item in quadrants['Q1'][:3]])}  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Q2: {quadrant_names['Q2']}     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ {chr(10).join([f'â€¢ {item[:12]}...' if len(item) > 12 else f'â€¢ {item}' for item in quadrants['Q2'][:3]])}  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â† {x_axis_label} â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ {x_axis_label} â†’
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Q3: {quadrant_names['Q3']}     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ {chr(10).join([f'â€¢ {item[:12]}...' if len(item) > 12 else f'â€¢ {item}' for item in quadrants['Q3'][:3]])}  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Q4: {quadrant_names['Q4']}     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ {chr(10).join([f'â€¢ {item[:12]}...' if len(item) > 12 else f'â€¢ {item}' for item in quadrants['Q4'][:3]])}  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**è±¡é™è¯´æ˜ï¼š**
- **Q1ï¼ˆå³ä¸Šï¼‰**ï¼šé«˜{x_axis_label}ï¼Œé«˜{y_axis_label} - ä¼˜å…ˆé‡ç‚¹
- **Q2ï¼ˆå·¦ä¸Šï¼‰**ï¼šä½{x_axis_label}ï¼Œé«˜{y_axis_label} - ä¸“ä¸šåˆ†æ
- **Q3ï¼ˆå·¦ä¸‹ï¼‰**ï¼šä½{x_axis_label}ï¼Œä½{y_axis_label} - åŸºç¡€ç»´æŠ¤
- **Q4ï¼ˆå³ä¸‹ï¼‰**ï¼šé«˜{x_axis_label}ï¼Œä½{y_axis_label} - åˆ›æ„åä½œ
"""

    return diagram

def main():
    """ä¸»å…¥å£ç‚¹"""
    logger.info("Starting Enhanced Article Quadrant Analyzer MCP Server")

    try:
        # ä½¿ç”¨stdioä¼ è¾“è¿è¡ŒMCPæœåŠ¡å™¨
        mcp.run()
    except KeyboardInterrupt:
        logger.info("Server stopped by user")
    except Exception as e:
        logger.error(f"Server error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()